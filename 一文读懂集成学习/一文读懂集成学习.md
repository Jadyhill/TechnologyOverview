# 一文读懂集成学习


标签： 机器学习 集成学习 

![](http://i.imgur.com/vjTLWu8.png)

##集成算法（Ensemble Algorithms）综述 ##

严格意义上来说，集成学习更像一种优化手段或者策略，集成学习通常是结合多个简单的弱机器学习算法，去做出更可靠的决策。有人把它称为机器学习中的“屠龙刀”，非常万能有效，集成模型是一种能在各种的机器学习任务上提高准确率的强有力技术，它往往是很多数据竞赛关键的一步，能够很好地提升算法的性能，是一种提高模型性能的强大方式。哲学思想为**“三个臭皮匠赛过诸葛亮”**。拿分类问题举个例，直观的理解，就是单个分类器的分类是可能出错，不可靠的，但是如果多个分类器投票，那可靠度就会高很多。

现实生活中，我们经常会通过投票，开会等方式，以做出更加可靠的决策。集成学习就与此类似。集成学习就是有策略的生成一些基础模型，然后有策略地把它们都结合起来以做出最终的决策。集成学习又叫多分类器系统。

集成方法是由多个较弱的模型集成模型组，一般的弱分类器可以是DT, SVM, NN, KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及如何结合的方法。这是一个非常强大的技术集，因此广受欢迎。

集成算法家族强大，思想多样，但是好像没有同一的术语，很多书本上写得也不一样， 不同的学者有不同的描述方式，最常见的一种就是依据集成思想的架构分为  Bagging ,Boosting, Stacking三种。国内，南京大学的周志华教授对集成学习有深入的研究，其在09年发表的一篇概述性论文[《Ensemple Learning》][1]对这三种架构做出了明确的定义。


##模型评价  

评价模型优劣的准则有很多。
欠拟合和过拟合是经常出现的两种情况，比较训练误差和测试误差的关系，当欠拟合时，可以设计更多特征来提升模型训练精度，当过拟合时，可以优化特征量降低模型复杂度来提升模型测试精度。

###过拟合与欠拟合

![](http://i2.muimg.com/567571/a964353260c46bd9.png)

###方差与偏差  

我们简单回顾一下模型的方差和偏差。模型的偏差指的是模型预测值和数据之间的差异，而方差指的是模型之间的差异，是度量基于同一观测值，预测值之间的差异。高偏差意味着我们的模型预测精度欠佳，高方差模型则意味着我们的模型可能发生了过拟合。



机器学习模型的Bias和Variance分析，[ Understanding the Bias-Variance Tradeoff][6]           中的一副图生动形象地为我们展示了偏差和方差的关系：

![](http://i1.piimg.com/567571/f068a39962edc16b.png)

![](http://i.imgur.com/M12InJd.png)

简单来说，一个模型越复杂，对训练样本的拟合度会越高，其Bias会越小（训练误差小）。但是，由于对数据过于敏感，生成的模型的变化范围可能比较大（Variance较大），可能导致在测试数据上性能的不确定性较高。


一个优秀的模型应该在偏差和方差之间保持平衡，这被称为偏差方差权衡问题。集成学习就是权衡这两种误差的一种方法。

集成学习的关键有两点：

1）如何构建具有差异性的分类器 ，  2）如何多这些分类器的结果进行整合。

基学习器之间的性能要有较大的差别，否则集成效果会很不好，也就是要保证多样性（Diversity），

基学习器从训练模型的样本数据集到模型的构造方法都会导致差异，可以是DT, SVM, NN, KNN等，也可以是训练过程不同的同一种模型，例如不同的参数，不同的训练集，或者不同的特征选择等。

## 常用的模型融合增强方法 ##

 - Bagging (Bootstrapped Aggregation)
 - Random Forest 
 - Boosting
 - AdaBoost （Adaptive Boosting）
 - Gradient Boosting Machines (GBM)梯度推进机
 - Gradient Boosted Regression Trees (GBRT)梯度提升回归树
 - Stacked Generalization (blending)层叠泛化

## 关于基学习器结果整合的常见方式 ##

### 1. 对于回归预测（数值预测）

 - （1）	简单平均（Simple Average），就是取各个分类器结果的平均值。

$$G(x) = \frac{1}{T}\sum\limits_{t = 1}^T {{g_t}(x)}$$  

 - （2）加权平均（Weighted Average）。

$$  G(x) = \frac{1}{T}\sum\limits_{t = 1}^T {{\alpha _t} \cdot {g_t}(x)} ,{\alpha _t} \ge 0 $$

### 2. 对于分类（类别预测）
 - （1）简单投票（Majority Voting）：就是每个分类器的权重大小一样，少数服从多数，类别得票数超过一半的作为分类结果 。
 $$ G(x) = sign\left( {\sum\limits_{t = 1}^T {1 \cdot {g_t}(x)} } \right) $$

 - （2）加权投票（Weighted Majority Voting）：每个分类器权重不一。
$$ G(x) = sign\left( {\sum\limits_{t = 1}^T {{\alpha _t} \cdot {g_t}(x)} } \right),{\alpha _t} \ge 0  $$

 - （3）概率投票（Soft vote）：有的分类器的输出是有概率信息的，因此可用概率投票。

##基于Bootstrap 的Bagging 算法


Bootstrap方法是非常有用的一种统计学上的估计方法。 Bootstrap是一类非参Monte Carlo方法,其实质是对观测信息进行再抽样，进而对总体的分布特性进行统计推断。首先，Bootstrap通过重抽样，避免了Cross-Validation造成的样本减少问题，其次，Bootstrap也可以创造数据的随机性。Bootstrap是一种有放回的重复抽样方法（可能抽到重复的样本），抽样策略就是简单的随机抽样。

Bagging（Bootstrapped Aggregation的简称）对于给定数据处理任务，通过bootstrap的方式获取多个训练集，采用不同的模型、参数、特征训练出多个模型，最后采用投票或者加权平均的方式输出最终结果。基学习器可以是相同的模型，也可以是不同的，一般使用的是同一种基学习器，最常用的是DT决策树。

Bagging通过对原数据集进行有放回的采样构建出大小和原数据集D一样的新数据集D1，D2，D3.....，然后用这些新的数据集训练多个分类器g1，g2，g3....。因为是有放回的采样，所以一些样本可能会出现多次，而有些样本会被忽略，理论上新的样本会包含67%的原训练数据。

![](http://i.imgur.com/ORsDJ3D.png)

Bagging通过降低基分类器的方差改善了泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于降低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么bagging方法就得不到性能的提升。

算法流程如下

![](http://i4.buimg.com/567571/cc43ef2b33cb5f47.png)

##基于Bagging的Random Forest

随机森林（Random Forest）是许多决策树的平均，每个决策树都用通过Bootstrap的方式获得随机样本训练。森林中的每个独立的树都比一个完整的决策树弱，但是通过将它们结合，可以通过多样性获得更高的整体表现。

随机森林会首先生成许多不同的决策树，每棵树变量的个数可以为\\( sqrt{K}  \\)（\\( K\\)为可用变量的个数），这样能够显著的加速模型的训练速度。一般的基础分类器的个数为500棵或者可以更多。

随机森林具有Self-testing的特性，因为随机森林是通过Bootstrap的方式采样，理论上往往会有大约1/3的原始数据没有被选中，我们叫做OOB（out of bag），而这部分数据刚好可以用来做测试，类似于Cross-Validation的作用。

随机森林是当今机器学习中非常流行的算法。它是一种“集群智慧”，非常容易训练（或构建），且它往往表现良好。

随机森林具有很多的优点 

 - 所有的数据都能够有效利用，而且不用人为的分出一部分数据来做cross-validation
 - 随机森林可以实现很高的精确度，但是只有很少的参数，而且对于分类和回归都适用
 - 不用担心过拟合的问题
 - 不需要事先做特征选择，每次只用随机的选取几个特征来训练树

   缺点

 -  相比于其他算法，其输出预测可能较慢。

##Boosting 

Boosting,基于错误提升分类器性能，通过集中关注被已有分类器分类错误的样本，构建新分类器并集成。其思想为模型每次迭代时通过调整错误样本的损失权重,提升对数据样本整体的处理精度。


Boosting与Bagging 相比来说，最大的区别就是 Boosting是串行的，而Bagging中所有的分类器是可以同时生成的，之间没有什么关系，而Boosting中则必须先生成第一个分类器，然后根据第一个分类器的结果生成第二个分类器，依次往后进行。

![](http://i.imgur.com/zz1SMrN.png)
![](http://i.imgur.com/dK9ks2E.png)


| 项目        | Bagging | Boosting  |
| --------   | -----:  | :----:  |
| 结构     | 并行 |   串行     |
| 训练集  |   独立  |  依赖 |
| 测试    | 可并行 |  需串行  |
| 作用    | 减小variance |  减小bias  |

核心思想是通过改变训练集进行有针对性的学习。通过每次迭代，增加错误样本的权重，减小正确样本的权重。知错就改，越来越好。

![](http://i4.buimg.com/567571/88eb224c50fadbde.png)

 上图（图片来自prml p660）就是一个Boosting的过程，绿色的线表示目前取得的模型（模型是由前m次得到的模型合并得到的），虚线表示当前这次模型。每次分类的时候，会更关注分错的数据，上图中，红色和蓝色的点就是数据，点越大表示权重越高，看看右下角的图片，当m=150的时候，获取的模型已经几乎能够将红色和蓝色的点区分开了。

算法流程如下

![](http://i2.muimg.com/567571/22ba88c3781d87b8.png)


增加前边学错样本的权重，减小已经判断正确的样本的权重，有点亡羊补牢，知错就改的意思，进行有针对性的学习。


理论上，Boosting 可以生成任意精确的分类器，而基础学习器则可以任意弱，只需要比瞎猜好一点就OK~


Bagging 减小方差 （variance ），而Boosting减小偏差（bias），关于具体的细节，可以参考知乎网友的解答[为什么说bagging是减少variance，而boosting是减少bias?][3]

##基于Boosting的AdaBoost

AdaBoost （Adaptive Boosting的简称）是Boosting中最具代表性的算法。AdaBoost 是一种Boosting方法，与Bagging不同的是，Adaboost中不同的子模型必须是串行训练获得，每个新的子模型是都根据已训练出的模型性能来进行训练的，而且Boosting算法中基学习器为弱学习,可以理解为只比随机猜测好一点，在二分类情况下，正确率略高于0.5即可。


AdaBoost中每个训练样本都有一个权重，初始值都为Wi=1/N。Adaboost中每次迭代生成新的子模型使用的训练数据都相同，但是样本的权重会不一样。AdaBoost会根据当前的错误率，按照增大错误样本权重，减小正确样本权重的原则更新每个样本的权重。不断重复训练和调整权重，直到训练错误率或基学习器的个数满足用户指定的数目为止。Adaboost的最终结果为每个弱学习器加权的结果。

算法流程如下

![](http://i4.buimg.com/567571/3a779b8c46d95276.png)

AdaBoost 优点： 

 - 很容易实施
 - 几乎没有参数需要调整 (我们的权重\\(\alpha _t\\)在训练过程中就被确定了）
 - 不用担心过拟合
 
      缺点：
 - 公式中的α是局部最优解，不能保证是最优解
 - 对噪声很敏感

## Gradient Boosting Machines (GBM)梯度推进机##


梯度提升和随机森林类似，都是由弱决策树构成的。最大的区别是，在梯度提升中树是一棵接一棵相继训练的。每个随后的树主要用被先前树错误识别的数据进行训练。这使得梯度提升更少地集中在容易预测的情况，而更多地集中在预测困难的情况。

它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数(loss function)描述的是模型的误差程度。如果我们的模型能够让损失函数持续的下降，则说明模型在不停断的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。

[Gradient Boost][4]与传统Boost的区别在于每个新的模型是为了使之前的模型的残差在梯度方向上减少。Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)。为了消除残差，每个新的模型的建立是为了使得之前模型的残差在梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

梯度提升训练速度也很快且表现非常好。然而，训练数据的小变化可以在模型中产生彻底的改变，因此它可能不会产生最可解释的结果。

##Gradient Boosted Regression Trees (GBRT)梯度提升回归树

首先应该说明的一点是，这个算法有很多名字，但其实是一样的~

 - Gradient Tree Boosting
 - GBRT (Gradient BoostRegression Tree) 梯度提升回归树 
 - GBDT (Gradient BoostDecision Tree) 梯度提升决策树 
 - MART (MultipleAdditive Regression Tree) 多决策回归树
 - Tree Net决策树网络


GBRT也是一种Boosting方法，每个子模型是根据已训练出的学习器的性能(残差)训练出来的树，子模型是串行训练获得，不易并行化。GBRT是基于残差的学习，没有AdaBoost中的样本权重的概念。GBRT结合了梯度迭代和回归树，准确率非常高，但是也有过拟合的风险。GBRT中迭代的是残差的梯度，残差就是目前结合所有得到的训练器预测的结果与实际值的差值。


GBRT使用非常广泛的，能分类，能回归预测。GBRT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。GBRT不是分类树而是回归树。

决策树分为回归树和分类树： 回归树用于预测实数值，如温度、用户年龄等。分类树用于分类标签值，如天气情况、用户性别等。和决策树相关的集成算法有两种比较基本的形式——随机森林与GBRT。

关于Bagging和Boosting架构下，集成算法更多细节，包括算法实现，可以参看Python官网机器学习包集成模块 [sk-learn][5]

##Stacking 

Wolpert在1992年的一篇[论文][2]中对 stacked generalization进行了介绍 , stacked generalization背后的基本思想是使用大量基分类器，然后使用另一种分类器来融合它们的预测，旨在降低泛化误差。

Stacking 主要分为两大部分。第一层就是传统的训练，训练出许多小分类器；第二层则是把这些小分类器的输出重新组合成为一个新的训练集，训练出来一个更高层次的分类器，目的就是寻找相应的权重或者它们之间的组合方式，第二层分类器的作用就是对基础分类器的输出进行集成。

![](http://i.imgur.com/L4BN6Gi.png)

算法流程如下

 ![](http://i2.muimg.com/567571/0fe40dd89c19420a.png)
 

Stacking 就像是 Bagging的升级版，Bagging中的融合各个基础分类器是相同权重，而Stacking中则不同,Stacking中第二层学习的过程就是为了寻找合适的权重或者合适的组合方式。

值得注意的是，在Stacking的架构下，有一些经常出现的说法如Stacking，  Blending , Stacked Generalization 很多文章也没有明确说明他们之间的关系。


如果不严格来区分的话，可以认为堆叠（Stacking），混合（Blending）和层叠泛化（Stacked Generalization）其实是同一种算法的不同名字罢了。在传统的集成学习中，我们有多个分类器试图适应训练集来近似目标函数。 由于每个分类器都有自己的输出，我们需要找到一个组合结果的组合机制，可以通过投票（大多数胜利），加权投票（一些分类器具有比其他权力更多的权威），平均结果等。

在堆叠中，组合机制是分类器（0级分类器）的输出将被用作另一个分类器（1级分类器）的训练数据，以近似相同的目标函数。基本上，让1级分类器找出集成机制。

>Stacking, Blending and and Stacked Generalization are all the same thing with different names. It is a kind of ensemble learning.
In traditional ensemble learning, we have multiple classifiers trying to fit to a training set to approximate the target function. Since each classifier will have its own output, we will need to find a combining mechanism to combine the results. This can be through voting (majority wins), weighted voting (some classifier has more authority than the others), averaging the results, etc. 

关于Stacking与Blending更多的细节可以参考 [KAGGLE ENSEMBLING GUIDE][26],中文 [kaggle比赛集成指南@qjgods][34]


最后我们也可以按照林轩田老师课程中的讲述来对机器学习集成算法做一个归纳。集成模型主要分为两条主线，一条Blending 线，一条 Learning线。Blending 假设我们已经得到了各个基础分类器  ，learning 主要指我们面对的是一堆数据，需要一边获得基础分类器 ，同时一边学习融合它们的方法。

![](http://i4.buimg.com/567571/9f8f2d51024fbb78.png)

**Blending框架**

![](http://i1.piimg.com/567571/ef211ac167f6938a.png)


**Learning框架**

![](http://i4.buimg.com/567571/33f7ba6295ccd970.png)

当然还有更加复杂一点的，就是**集成的集成**。

![](http://i1.piimg.com/567571/a2c470e4bd193e6c.png)


##资源
###机器学习基石&技法 台湾大学 林轩田老师

这两门课是台大林轩田老师开设的机器学习入门课。基石课注重理论，涵盖了 VC Dimension、Overfitting、Regularization、Validation 等非常基本的问题，；技法课则将 SVM、AdaBoost、Decision Tree、Random Forest、Deep Learning、RBF Network 等大量实际算法分成三类加以介绍，每个算法都讲的较深，有大量的数学推导，并且非常注重模型、算法之间的相互联系。课程幻灯片制作精美，讲课深入浅出。它的作业很具有挑战性，需要花费大量的时间。
关于集成学习（aggregation），林轩田老师在技法课中设为lecture 7 到lecture 11，大概五个小时，可以全面的了解集成算法的相关细节，以及背后的数学原理，可以更好的在实践中运用集成算法。

[课程视频和讲义][7]

[讨论区][8]

[参考用书][9]

[作业参考][10]

### 数据挖掘：理论与算法（袁博）学堂在线版
[数据挖掘课程][11]

银河系的后裔，袁博老师，明明是实力派，却偏要走偶像路线。 袁博老师生动幽默的讲述了数据挖掘中的核心思想、关键技术以及一些在其它相关课程和教科书中少有涉及的重要知识点。关于集成学习的模块在课程的第八周。

![](http://i4.buimg.com/567571/34802d1fc7f37ea0.png)

###书籍推荐

![](http://i.imgur.com/O3dXxNG.jpg)


![](http://i.imgur.com/v7q9o6m.jpg)

###扩展阅读
[XGBoost 与 Boosted Tree  陈天奇 ][12]

[XGBoost: A Scalable Tree Boosting System][13]

[Ensample learning][14]

[Ensample][15]

[Boosting][16]

[AdaBoost][17]

[机器学习算法中GBDT和XGBOOST的区别有哪些？][18]

[如何评价周志华教授新提出的 Deep Forest 模型，它会取代当前火热的深度学习 DNN 吗？][19]

[ENSEMBLE METHODS FOR CLASSIFIERS ][20]

[Data Science Knowledge Repo][21]

[A Comparison of Model Aggregation Methods for Regression][22]

[Implementation of Breiman’s Random Forest Machine Learning Algorithm ][23]

[Classification and Regression by random Forest][24]

[40 Questions to ask a Data Scientist on Ensemble Modeling Techniques (Skilltest Solution)][25]

[KAGGLE ENSEMBLING GUIDE][26]



####本文参考
[学习 ensemble learning 要如何开始呢？][27]

[机器学习中的算法(1)-决策树模型组合之随机森林与GBDT  @LeftNotEasy][28]

[机器学习中的数学(3)-模型组合(Model Combining)之Boosting与Gradient Boosting@LeftNotEasy][29]

[使用sklearn进行集成学习——理论@jasonfreak][30]

[集成学习方法][31]

[Ensemble learning（集成学习@GJS Blog）][32]

[机器学习技法 --- 融合模型@罐装可乐][33]

[kaggle比赛集成指南@qjgods][34]

[Stacking, Blending and Stacked Generalization][35]


  [1]: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf
  [2]: https://www.researchgate.net/publication/222467943_Stacked_Generalization
  [3]: https://www.zhihu.com/question/26760839
  [4]: https://en.wikipedia.org/wiki/Gradient_boosting
  [5]: http://scikit-learn.org/stable/modules/ensemble.html
  [6]: http://scott.fortmann-roe.com/docs/BiasVariance.html
  [7]: http://www.csie.ntu.edu.tw/~htlin/mooc/
  [8]: https://www.csie.ntu.edu.tw/~htlin/course/ml15fall/
  [9]: http://book.caltech.edu/bookforum/
  [10]: http://blog.csdn.net/a1015553840/article/details/51085129#reply
  [11]: http://www.xuetangx.com/courses/course-v1:TsinghuaX+80240372X+sp/about
  [12]: http://www.52cs.org/?p=429
  [13]: https://arxiv.org/abs/1603.02754
  [14]: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf
  [15]: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/SpringerEDBS09a.pdf
  [16]: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/SpringerEDBS09b.pdf
  [17]: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/top10chapter.pdf
  [18]: https://www.zhihu.com/question/41354392
  [19]: https://www.zhihu.com/question/56474891
  [20]: https://datajobs.com/data-science-repo/Ensemble-Methods-%5BLior-Rokach%5D.pdf
  [21]: https://datajobs.com/data-science-repo/
  [22]: https://datajobs.com/data-science-repo/Boosting-and-Bagging-%5BBarutcuoglu-and-Alpaydin%5D.pdf
  [23]: https://datajobs.com/data-science-repo/Random-Forest-%5BFrederick-Livingston%5D.pdf
  [24]: https://datajobs.com/data-science-repo/Random-Forest-%5BLiaw-and-Weiner%5D.pdf
  [25]: https://www.analyticsvidhya.com/blog/2017/02/40-questions-to-ask-a-data-scientist-on-ensemble-modeling-techniques-skilltest-solution/
  [26]: http://mlwave.com/kaggle-ensembling-guide/
  [27]: https://www.zhihu.com/question/29036379
  [28]: http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html
  [29]: http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/machine-learning-boosting-and-gradient-boosting.html
  [30]: http://www.cnblogs.com/jasonfreak/p/5657196.html
  [31]: http://www.36dsj.com/archives/63032
  [32]: http://www.cnblogs.com/GuoJiaSheng/p/4033584.html
  [33]: http://www.cnblogs.com/daguankele/p/6486615.html
  [34]: http://blog.csdn.net/a358463121/article/details/53054686
  [35]: http://www.chioka.in/stacking-blending-and-stacked-generalization/